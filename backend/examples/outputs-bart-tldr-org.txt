FearNet is a generative model that does not store previous examples, making it memory efficient. 
Multi-view learning can provide self-supervision when different views are available of the same data. 
We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation. 
We extend a state-of-the-art attention network and demonstrate the benefit of using stronger supervisory signals by teaching DCNs to attend to image regions that humans deem important for object recognition. 
We show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial example. 
We show that there is surprisingly little difference between various deep learning architectures in terms of accuracy, prompting a preference for the simpler architectures, since they are faster to train and less prone to overfitting. 
We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations. 
We present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. 
We propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. 
In this paper, we propose to use ridge regression and logistic regression as the main adaptation mechanism for few-shot learning. 
We propose active learning with partial feedback (ALPF), where the learner must actively choose both which example to label and which binary question to ask. 
Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. 
We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly. 
We propose a new pruning method, spatial-Winograd pruning, which efficiently transfers the spatial-domain sparsity into the Winograd domain. 
We develop a Bayesian nonparametric framework for federated learning with neural networks. 
A general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. 
This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. 
A simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. 
We introduce a generative model called Conditional Relationship Variational Autoencoder (CRVAE), which can discover meaningful and novel relational medical entity pairs without the requirement of additional external knowledge. 
A simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. 
We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. 
Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. 
We explore a new dimension of the design space: quantizing different layers with different bit-widths. 
We introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size. 
We introduce Mol-CycleGAN, a CycleGAN-based model that generates optimized compounds with a chemical scaffold of interest. 
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. 
We introduce a novel ranking loss function tailored for RNNs in recommendation settings. 
In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. 
We propose the normalized direction-preserving Adam algorithm, which controls the update direction and step size more precisely, and thus bridges the generalization gap between Adam and SGD. 
In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. 
We approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets. 
We introduce a novel, biologically inspired visual working memory architecture that we term the Hebb-Rosenblatt memory. 
In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. 
We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. 
Cramer-Wold AutoEncoder for Wasserstein Autoencoders. 
We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. 
We propose a simple and fast way to train supervised convolutional models to feature extraction while still maintaining its high-quality. 
We develop a framework for understanding and improving recurrent neural networks (RNNs) using max-affine spline operators. 
In this paper, we first propose Neighbourhood-approximated Neural Theorem Provers (NaNTPs), a method for drastically reducing the previously prohibitive time and space complexity during inference and learning, and an attention mechanism for improving the rule learning process, deeming them usable on real 
We investigate the methods by which a Reservoir Computing Network learns concepts such as'similar' and 'different' between pairs of images using a small training dataset and generalizes these concepts to previously unseen types of data. 
We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations of the data. 
We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. 
We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents. 
This paper proposes a neural end-to-end text- to-speech model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. 
We propose a neural network component that allows robust counting from object proposals. 
Inspired by the Gram-Schmidt Process in geometric theory, we develop an orthogonal basis to combine pre-trained word embeddings into sentence representation. 
We propose novel extensions of Prototypical Networks that are augmented with the ability to use unlabeled examples when producing prototypes. 
We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. 
We proposed an end-to-end DNN for abnormality detection in medical imaging. 
We show that DRL algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage. 
End-to-end training through differentiable Bayesian Filters enables us to learn more complex heteroscedastic noise models for the system dynamics. 
We propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. 
We propose a capsule-based neural network model to solve the semantic segmentation problem. 
We propose a framework for understanding SGD learning in the information plane which consists of observing entropy and conditional entropy of the output labels of ANN. 
This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. 
We propose a new, multi-component energy function for energy-based Generative Adversarial Networks. 
We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different damping coefficients. 
We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past. 
In this paper, we introduce variance layers, a different kind of stochastic layers. 
In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work. 
In this paper we propose a cheap pruning algorithm based on difference of convex (DC) optimisation. 
In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal Convolutional kernels that capture the local motion changes of different actions; the decoder is a 
In this work, we argue that feature is the most important knowledge from teacher. From this discovery, we further present an efficient learning strategy to mimic features stage by stage. 
We improve adversarial robustness by 11% over the current state- of-the-art result in the `2-norm on CIFAR-10. 
A neural network based model for Reading Comprehension with Multiple Choice Questions. 
In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. 
We propose a method to reduce the communication overhead of distributed deep learning. 
In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. 
We propose a variant of the back-propagation algorithm, "Conceptor-Aided Backprop", in which gradients are shielded by conceptors against degradation of previously learned tasks. 
We propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses. 
We present code2seq: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. 
We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. 
We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location. 
Online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. 
We propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training. 
The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastically gradient in its updates. We establish convergence rates for signSGD on general non-convex functions under transparent conditions. 
We introduce a transparent middleware layer for neural network acceleration. 
We propose a simple, efficient convolutional scheme that encodes rotational equivariance, along with a method for integrating the magnitude response of the 2D-DFT to encode global rotational invariance. 
NeuroFovea metamer model, a foveated generative model that is based on a mixture of peripheral representations and style transfer forward-pass algorithms. 
We show that over-parametrization improves the normalized margin and generalization error bounds for deep networks, and that perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time. 
We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. 
In this article, we analyze neural networks from a frame theoretic perspective to identify the sufficient conditions that enable smoothly recoverable representations of signals in L^2(R). 
In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. 
We investigate the overconfidence problem and propose methods that reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time. 
We present a large batch, stochastic optimization algorithm that is faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. 
We propose a method to train models whose weights are a mixture of bitwidths, that allows us to more finely tune the accuracy/speed trade-off. 
Bias propagation is a pruning technique which consistently outperforms the common approach of merely removing units,  regardless of the architecture and the dataset. 
We develop an alternative that utilizes the most powerful generative models as decoders, optimize the variational lower bound, and ensures that the latent variables preserve and encode useful information. 
We propose a batch adaptive stochastic gradient descent that can dynamically choose a proper batch size as learning proceeds. 
We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. 
We show that modular models that generalize well could be made more end-to-end by learning their layout and parametrization. 
We introduce Relational Forward Models (RFM) for multi-agent learning, networks that can learn to make accurate predictions of agents' future behavior in multi- agent environments. 
We show that gradient descent on an unregularized logistic regression problem, for almost all separable datasets, converges to the same direction as the max-margin solution. 
We introduce the gray-level co-occurrence matrix to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. 
In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. 
We introduce the differentiable decision tree (DDT) as a modular component of deep networks and a simple, differentiable loss function that allows for end-to-end optimization of a deep network to compress high-dimensional data for classification by a single decision tree. 
Regularized Learning under Label shifts (RLLS), a principled and a practical domain-adaptation algorithm to correct for shifts in the label distribution between a source and a target domain. 
We propose a method that discriminatively learns an embedding in which a simple Bayesian classifier can balance the class-priors to generalize well for rare classes. 
In this work we present a method for synthesizing states of interest for a trained agent. 
We introduce the deep abstaining classifier, a deep neural network trained with a novel loss function that provides an abstention option during training. 
We propose a novel TD learning method, Hadamard product Regularized TD (HR-TD), that reduces over-generalization and thus leads to faster convergence. 
We present an efficient convolution kernel for CNNs on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. 
We decouple visual prediction from a rigid notion of time. 
In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. 
Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. 
Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. 
We propose a methodology for extending training datasets to arbitrarily big sizes and training complex, data-hungry models using weak supervision. 
We introduce contextual explanation networks that learn to predict by generating and leveraging intermediate explanations. 
We propose a model free, off-policy IL algorithm for continuous control. 
A novel graph convolutional network that generalizes CNN architectures to graph-structured data and provides a systematic way to design a set of learnable filters to perform convolutions on graphs. 
Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. 
We present an unsupervised approach for learning disentangled representations of objects entirely from unlabeled monocular videos. 
Learning from a scalar reward in continuous action space environments is difficult and often requires millions if not billions of interactions. 
We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. 
In this work we introduce a novel Siamese Deep Neural Network architecture that is able to effectively learn from data in the presence of multiple adverse events. 
We present a system that allows for querying data tables using natural language questions, where the system translates the question into an executable SQL query. 
To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. 
We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD. 
We propose to perform a soft-clustering of data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. 
We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers. 
We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities and observations. 
We propose AlignFlow, a framework for unpaired cross-domain translation that ensures exact cycle consistency in the learned mappings. 
In this paper we address this challenge by constructing a representative subset of examples that is both small and able to constrain the solver sufficiently. 
We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. 
We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner, but it becomes possible given two sets of U data with different class priors. 
We introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. 
Somatic cancer mutation detection at ultra-low variant allele frequencies (VAFs) with convolutional architecture. 
This paper presents the formal release of MedMentions, a new manually annotated resource for the recognition of biomedical concepts. 
In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm. 
We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. 
We propose in this paper a new approach to train the Generative Adversarial Nets with a mixture of generators to overcome the mode collapsing problem. 
We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. 
We propose a novel algorithm that jointly learns and compresses a neural network. 
We show that naive application of the SVRG technique and related approaches fail, and explore why. 
We present a method for applying deep learning to 3D surfaces using their spherical descriptors and alt-az anisotropic convolution on 2-sphere. 
We introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. 
Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. 
An efficient federated learning framework based on variational dropout. 
We prove a multiclass boosting theory for the ResNet architectures and provide a new algorithm for ResNet-style architectures. 
We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \sigma(Bx) + \xi$ is a noise vector. 
We release, describe, and analyze an OIE corpus called OPIEC, which was  extracted from the text of English Wikipedia. 
We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width. 
We develop a new method termed as ``"WAGE" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. 
In this paper, we explore the sparsification of CNNs by proposing three model-independent methods. 
In this paper we experimented that we can also obtain good results by adding the samples randomly without a meaningful order. 
We study the problem of learning to map, in an unsupervised way, between domains $A$ and $B$, such that the samples $\vb \in B$ contain all the information that exists in samples $\va\in A$ and some additional information. 
In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. 
We propose new architectures that use a sparser coupling between the channels and thereby reduce both the number of trainable weights and the computational cost of the CNN. 
We use rate-distortion theory, a branch of information theory devoted to the problem of lossy compression, to shed light on an important problem in latent variable modeling of data: is there room to improve the model? 
Learning with Linear Backprop is competitive with Backprop and saves expensive gradient computations. 
In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. 
We present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. 
We propose deep net triage, which individually assesses small blocks of convolution layers to understand their collective contribution to the overall performance. 
In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. 
We derive a novel weight initialisation scheme for standard, finite-width networks that takes into account the structure of the data and information about the task at hand. 
Working memory requires information about external stimuli to be represented in the brain even after those stimuli go away. 
We propose a model that, on one hand, exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior), but on the other hand, is sufficiently simple to enable rigorous convergence analysis. 
We study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting. 
In this paper we present a variational framework for Bayesian phylogenetic analysis. 
This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive models for raw audio waveform generation. 
We propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. 
In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. 
We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. 
We propose a novel algorithm, Deep Temporal Clustering (DTC), a fully unsupervised method to naturally integrate dimensionality reduction and temporal clustering into a single end to end learning framework. 
We study many-class few-shot (MCFS) problem in both supervised learning and meta-learning scenarios. 
We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. 
In high dimensions, the performance of nearest neighbor algorithms depends crucially on structure in the data. 
We introduce a differentiable quantization procedure for neural networks that can be effectively discretized without loss of performance. 
We propose distributional adversaries that operate on samples, i.e., on sets of multiple points drawn from a distribution, rather than on single observations. 
We propose a framework to do the auto standardization from the non-systematic names to the corresponding systematic names by using the spelling error correction, byte pair encoding tokenization and neural sequence to sequence model. 
We extend previous work by investigating the curvature of the loss surface along the whole training trajectory, rather than only at the endpoint. 
A new approach to estimate continuous actions using actor-critic algorithms for reinforcement learning problems. 
We propose a specialized dropout method for DenseNet. 
We propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples. 
We show that neural networks with binary weights and activations work because of the high-dimensional geometry of binary vectors. 
In this paper, we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. 
We consider the learning of algorithmic tasks by mere observation of input-output pairs, and study what are its implications in terms of learning. 
We propose a General and One-sample gradient that applies to many distributions associated with non-reparameterizable continuous random variables, yielding statistical back-propagation, coupling neural networks to common random variables. 
We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer. 
A general objective function to adapt the robust training method of Wong & Kolter (2018) to optimize for cost-sensitive robustness. 
We propose a method to learn a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs). 
We introduce a novel workflow, QCue, for providing textual stimulation during mind-mapping. 
We propose a simple and efficient plug-and-play detection procedure that does not require re-training, pre-processing or changes to the model. 
We propose the Maximal Divergence Sequential Auto-Encoder for binary code vulnerability detection. 
In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output tokens across a predicted sequence. 
We propose a technique that directly minimizes both the model complexity and the changes in the loss function during compression. 
We present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. 
The design of small molecules with bespoke properties is of central importance to drug discovery, and it can be difficult to compare results across different works. 
We study how analogical reasoning can be induced in neural networks that learn to perceive and reason about raw visual data. 
We propose a new setting in goal-oriented dialogue system to tighten the gap between these two aspects by enforcing model level information isolation on individual models between two agents. 
We propose to use unsupervised deep language models to complete and correct the queries given an arbitrary prefix. 
We provide proofs that ADAM and RMSProp are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time. 
We introduce a novel automated countermeasure called Parallel Checkpointing Learners (PCL) to thwart the potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model. 
We present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. 
In this paper, we design a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}. 
In this work, we address the problem of learning an agentâ€™s action space purely from visual observation. 
We propose a framework for training agents to negotiate and form teams using deep reinforcement learning. 
Neural machine translation models learn representations containing substantial linguistic information, but it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. 
We propose a framework that disentangles task and environment specific knowledge by separating them into two units. 
We propose pix2scene, a deep generative based approach that implicitly models the geometric properties of a scene from images. 
We model relation representation as a supervised learning problem and learn parametrised operators that map pre-trained word embeddings to relation representations. 
We propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. 
We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. 
This is an empirical paper which constructs color invariant networks and evaluates their performances on a realistic data set. 
We show that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. 
We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks. 
We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. 
We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. 
We develop new algorithms for estimating heterogeneous treatment effects, combining recent developments in transfer learning for neural networks with insights from the causal inference literature. 
We propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. 
In this paper, we address such optimal assumption by learning only from the most suitable demonstrations in a given set. 
We introduce causal implicit generative models (CiGMs), models that allow sampling from not only the true observational but also the true interventional distributions. 
Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. 
We use affect scores from Warriner's affect lexicon to improve the word representations learnt from an unlabelled corpus. 
An unsupervised method that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias as a representation expectation for each node in the given graph. 
We argue that the generalization performance of linear graph embedding methods is not due to the dimensionality constraint as commonly believed, but rather the small norm of embedding vectors. 
We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. 
We propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. 
We proposed a new driving model which is composed of perception module for see and think and driving module for behave, and trained it with multi-task perception-related basic knowledge and driving knowledge stepwisely. 
We introduce the MultI-Level Embedding (MILE) framework, a generic methodology allowing contemporary graph embedding methods to scale to large graphs. 
We propose a defense mechanism that is based on a contraction of the data, and we test its effectiveness using OCSVMs. 
In this paper, we present a layer-wise learning of stochastic neural networks in an information-theoretic perspective. 
The maximum mean discrepancy between two probability measures P and Q is a metric that is zero if and only if all moments of the two measures are equal, making it an appealing statistic for two-sample tests. 
We propose Bayesian Deep Q-Network, a practical Thompson sampling based Reinforcement Learning (RL) Algorithm. 
We propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN. 
We propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. 
In our work, we adapt notions of similarity using weak labels over multiple hierarchical levels to boost classification performance. 
We propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative advantage function and is robust to partially observed state. 
We propose a novel latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL). 
Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. 
We propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. 
In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem. 
We propose a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints. 
We present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. 
We introduce an analytic distance function for moderately sized point sets of known cardinality that has very desirable properties, both as a loss function and a regularizer for machine learning applications. 
We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. 
We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. 
This paper introduces the concept of continuous convolution to neural networks and deep learning applications in general. 
We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence. 
In this work, we exploited different strategies to provide prior knowledge to commonly used generative modeling approaches aiming to obtain speaker-dependent low dimensional representations from short-duration segments of speech data, making use of available information of speaker identities. 
In this work we provide yet another perspective on the IWAE bounds on the marginal likelihood of latent variable models. 
In this paper, we consider the problem of autonomous lane changing for self driving vehicles in a multi-lane, multi-agent setting. 
We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. 
We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. 
In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. 
We present a personalized recommender system using neural network for recommending products, such as eBooks, audio-books, Mobile Apps, Video and Music. 
We propose to resolve image restoration challenges by coupling the GAN based image restoration framework with another task-specific network. 
We present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. 
We introduce the Projective Subspace Networks (PSN), a deep learning paradigm that learns non-linear embeddings from limited supervision. 
This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. 
In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images. 
We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations. 
We define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. 
We show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the L STM computational graph get suppressed, which prevents LSTMs from capturing long term dependencies. 
We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. 
We show that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples. 
We present Gedit, a system of on-keyboard gestures for convenient mobile text editing. 
This paper attempts to provide an alternative understanding of deep learning generalization from the perspective of maximum entropy. 
We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. 
We show CNNs are more robust to class-dependent label noise than class-independent label noise. 
We demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies. 
In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation. 
We propose WAAT, a 3D authoring tool allowing user to quickly create 3D models of the assembly line and also test the AR guidance placement. 
We show that, with a proper stepsize choice, the widely used first-order iterative algorithm in training GANs would in fact converge to a stationary solution with a sublinear rate. 
We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). 
We propose dithered quantization for the transmission of the stochastic gradients and show that training with Dithered Quantized Stochastic Gradients (DQSG) is similar to the training with unquantized SGs perturbed by an independent bounded uniform noise, in contrast to 
In this paper we propose a new defensive mechanism under the generative adversarial network~(GAN) framework. 
We propose to use ensemble methods as a defense strategy against adversarial perturbations. 
In this paper, we propose the Associative Conversation Model that generates visual information from textual information and uses it for generating sentences in a dialogue system without image input. 
In this work, we designed a Contextual Recurrent Convolutional Network with this feature embedded in a standard CNN structure. 
We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty. 
 gradient dropping with local gradients approaches convergence 48% faster than non-compressed multi-node training and 28% faster compared to vanilla gradient dropping. 
We show that the density of the Q function estimated by Distributional RL can be successfully used for the estimation of UCB. 
We introduce community-based autoencoders in which multiple encoders and decoders collectively learn representations by being randomly paired up on successive training iterations. 
This paper introduces the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. 
We extend batch normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features. 
We propose a distillation-based approach to boost the accuracy of multilingual machine translation. 
This paper investigates the role of human priors for solving video games. 
We propose the use of $k$-determinantal point processes in hyperparameter optimization via random search. 
In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. 
Block diagonal inner product layers can be achieved by either initializing a purely block diagonal weight matrix or by iteratively pruning off diagonal block entries. 
We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. 
We propose a method to learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. 
In this paper, we develop a new theoretical framework to analyze one and two dimensional GRUs as a continuous dynamical system, and classify the dynamical features obtainable with such system. 
We propose a multi-scale stacked hourglass network to high-light the differentiation capabilities of each Hourglass network for human pose estimation. 
We present a new unsupervised method for learning general-purpose sentence embeddings. 
We explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. 
We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust to adversarial attacks. 
We introduce a model that avoids the autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. 
We propose a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. 
We formulate an information-based optimization problem for supervised classification. 
We propose an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoen coder. 
This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions. 
We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. 
A zero-shot method for imitation learning in which an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. 
In this paper we provide comparison between two methods for post process improvements to the baseline DSM vectors. 
We quantize the network, both weights and activations, into multiple binary codes {-1,+1}. 
A method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). 
We propose a single shot analysis of a trained CNN that uses Principal Component Analysis (PCA) to determine the number of filters that are doing significant transformations per layer, without the need for retraining. 
This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. 
A new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. 
A new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output. 
This paper demonstrates a novel approach, efficiently implementing many deep learning functions with bootstrapped homomorphic encryption. 
In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. 
We show that weight-quantized models converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits 
We show that imposing sparsity at the level of the representation (i.e. neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. 
A Synaptic Neural Network (SynaNN) consists of synapses and neurons. 
We conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework. 
We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning. 
We propose implicit causal models, a class of causal models that leverages neural architectures with an implicit density. 
Few-shot learning trains image classifiers over datasets with few examples per category. 
We show that using pre-trained embeddings on code tokens provides the same benefits as it does to natural languages, achieving: over 1.9x speedup, 5\% improvement in test loss, 4\% improved F1 scores, and resistance to over-fitting. 
Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. 
We introduce an end-to-end differentiable model for interpreting questions, inspired by formal approaches to semantics. 
We present DLVM, a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. 
We develop an attention mechanism for multi-modal fusion of visual and textual modalities that allows the agent to learn to complete the navigation tasks and also achieve language grounding. 
We propose a new Q\&A architecture called QANet, which does not require recurrent networks:  its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and selfattention models global interactions. 
In this paper we introduce the building blocks for constructing spherical CNNs. 
We propose a novel method that makes use of deep neural networks and gradient decent to perform automated design on complex real world engineering tasks. 
We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy. 
Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities  required for the forward propagation of CNNs, in a binary file less  than 400KB. 
We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. 
We introduce hierarchically clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. 
We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers. 
We propose a hierarchical approach to structured exploration to improve the sample efficiency of on-policy exploration in large state-action spaces. 
We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. 
We conduct a mathematical analysis on the Batch normalization (BN) effect on gradient backpropagation in residual network training. 
To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. 
We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. 
We propose to learn a proper prior from data for adversarial autoencoders (AAEs). 
Using Skip-Thought sentence embeddings in conjunction with GANs for text generation. 
The novel \emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. 
We present a deep learning-based method for super-resolving coarse (low-resolution) labels assigned to groups of image pixels into pixel-level (high-resolution), given the joint distribution between those low- and high-resolution labels. 
We propose a novel framework for combining datasets via alignment of their associated intrinsic dimensions. 
We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure. 
We introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. 
We propose a novel `"volumetric convolution" operation that can effectively convolve arbitrary functions in B^3. 
We propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. 
We propose a method for sharing label information across languages by means of a language independent text encoder. 
We propose the deep inside-outside recursive autoencoder (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. 
We show short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes. 
In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic represen- tation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom 
We propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. 
In this paper, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations. 
A method to collapse hyperparameter optimization into joint stochastic optimization of both weights and hyperparameters. 
We propose a novel covariance estimator based on the Gaussian Process Latent Variable Model (GP-LVM). 
We study how, in generative adversarial networks, variance in the discriminator's output affects the generator's ability to learn the data distribution. 
We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agentâ€™s policy can be used to aid efficient exploration. 
We propose Active Neural Localizer, a fully differentiable neural network that learns to localize efficiently. 
In this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. 
In this paper we are proposing to replace the basic linear combination operation with non-linear operations that do away with the need of additional non- linear activation function. 
We propose the Integral Pruning (IP) technique which integrates the activation pruning with the weight pruning. 
In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations. 
DANA, a novel approach that scales out-of-the-box to large clusters using the same hyperparameters and learning schedule optimized for training on a single worker, while maintaining similar final accuracy without additional overhead. 
This paper proposes a novel approach to train deep neural networks by unlocking the layer-wise dependency of backpropagation training. 
Anticipated Reweighted Truncated Backpropagation, an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. 
A greedy algorithm is proposed to generate adjacency and feature matrices of fake nodes, aiming to minimize the classification accuracy on the existing ones. 
In this paper, we proposed the aligned recurrent transfer, ART, to achieve cell-level information transfer. 
We propose a new policy network architecture that encodes the belief distribution independently from the observable state. 
For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large 
In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. 
We discuss the feasibility of a learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. 
We present a novel approach for certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming. 
A comprehensive empirical study to provide insights into the interplay between expressivity and interpretability in this model family with respect to language modeling and parts-of-speech induction. 
In this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement. 
This report introduces a training and recognition scheme, in which classification is realized via class-wise discerning. 
A long-held conventional wisdom states that larger models train more slowly when using gradient descent. 
We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. 
We show that convolutional neural networks are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms. 
We develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. 
The first systematic survey of when local codes emerge in a feed-forward neural network. 
We propose a multimodal embedding using different neural encoders for this variety of data, and combine with existing models to learn embeddings of the entities. 
An ensemble of neural networks is known to be more robust and accurate than an individual network, however usually with linearly-increased cost in both training and testing. 
This paper introduces a new framework for data efficient and versatile learning. 
We propose a new negative sampling and weighting scheme which we denote as Cooperative Importance Sampling (CIS). 
In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation. 
This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. 
We present PEARL,  Prototype lEArning via Rule Lists, which iteratively uses rule lists to guide a neural network to learn representative data prototypes. 
We propose the Deli-Fisher GAN, a GAN that generates photo-realistic images by enforcing structure on the latent generative space. 
We propose a new modular network architecture that applies an attentional mechanism not on temporal and spatial regions of the input, but on sensor selection. 
We propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud. 
We propose a Recurrent GAN and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. 
We developed a model from the data in our first study, and used it to predict the results in the second; the model was accurate, with high correlations between predictions and real values. 
We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. 
We propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. 
We train quantized neural networks by noise injection and learned clamping, which improve the accuracy. 
In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. 
We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. 
This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks, including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd Convolution, and 3) topology-wise 8- bit support. 
We extend hyperbolic geometry on the embeddings used to compute the ubiquitous attention mechanisms for different neural networks architectures. 
We present a method for evaluating the sensitivity of deep reinforcement learning (RL) policies. 
In this paper, we provide a quantitative analysis of the robustness of deep networks to universal perturbations, and draw a formal link to the geometry of the decision boundary. 
We propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. 
We show that modularity between activation maps distributed over channels of generator architectures is achieved to some degree, can be used to better understand how these systems operate. 
We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. 
We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks. 
We propose a method which can visually explain the classification decision of deep neural networks (DNNs). 
We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping. 
We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversARially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model. 
We introduce the routing network paradigm, a novel neural network and training algorithm. 
We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. 
We propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. 
We propose a generative autoencoder based on the idea of reversible generative models to unrestricted neural network architectures and arbitrary latent dimensionalities. 
We introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold. 
We propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. 
We explicitly model sparsity in the latent space of a VAE with a Spike and Slab prior distribution. With the new approach, we are able to infer truly sparse representations with generally intractable non-linear probabilistic models. 
In this work we propose a simple method to address the degradation problem in deep networks where the presence of skip-connections is penalized by Lagrange multipliers. 
We propose methods to reduce the memory footprint of deep learning models by up to 60$\times$ without affecting their test accuracy. 
We train a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. 
In this paper, we propose deep convolutional generative adversarial networks that learn to produce a'mental image' of the input image as internal representation of a certain category of input data distribution. 
We present a framework that combines techniques in formal methods and hierarchical reinforcement learning. 
We propose a method for weight and activation quantization that is scalable in terms of quantization levels (n-ary representations) and easy to compute while maintaining the performance close to full-precision CNNs. 
We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. 
We identify the atomic building blocks of existing methods, and decouple the assumptions they enforce from the mathematical tools they rely on. 
We prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of deep convolutional networks that correspond to the Tensor Train (TT) decomposition. 
We propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. 
In the adversarial-perturbation problem of neural networks, an adversary starts with a neural network model $ F$ and a point $\bfx$ that $F$ classifies correctly, and applies a \emph{small perturbation} to  $\BFx$ to produce another 
This paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method. 
We propose two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. 
A set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. 
We explore a method to improve the sample efficiency when we have access to demonstrations. 
We propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states. 
We achieve bias-variance decomposition for Boltzmann machines using an information geometric formulation. 
We design an efficient implementation for sparse RNNs.   We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and bank-aware weight layout. 
In this paper, a new sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. 
This paper proposes a novel framework for efficient multi-task reinforcement learning. 
We propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology. 
Learning deep networks which can resist large variations between training andtesting data is essential to build accurate and robust image classifiers. 
We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples. 
We prove dimension-independent bounds for low-precision training algorithms that use fixed-point arithmetic. 
We show E-MAML and ERL2 deliver better performance on tasks where exploration is important. 
We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs. 
We demonstrate how ground truths can serve to assess the effectiveness of attack techniques, by comparing the adversarial examples produced by those attacks to the ground truths. 
This paper introduces a new framework for open-domain question answering in which the retriever and the reader \emph{iteratively interact} with each other. 
This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution. 
We propose a novel neural network that is able to generate topic-specific questions. 
We introduce a new computational approach that decodes movement intent from a low-dimensional latent representation of the neural data. 
We apply experimental paradigms from developmental psychology to a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. 
We propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. 
We perform an early systematic study of system resource efficiency for super-resolution (SR). 
We show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. 
We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle. 
We introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. 
We explore ways of incorporating bilingual dictionaries to enable semi-supervised neural machine translation. 
We propose a new curiosity method which uses episodic memory to form the novelty bonus. 
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. 
We propose an efficient training methodology and incrementally growing a DCNN to allow new classes to be learned while sharing part of the base network. 
We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. 
We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples. 
We show that human-vision-inspired texture representations can outperform CNN-encoded features in detecting visual anomalies. 
A new type of regularization approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector. 
We prove that learnable gates in a recurrent model formally provide \emph{quasi-invariance to general time transformations} in the input data. 
In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. 
We present DL2, a system for training and querying neural networks with logical constraints. 
We present Genetic Policy Optimization (GPO), a genetic algorithm for sample-efficient deep policy optimization. 
We show that the straight-through gradient method is in fact identical to the well-known Nesterovâ€™s dual-averaging algorithm on a quantization constrained optimization problem. 
We prove that, with high probability in the limit of N\rightarrow\infty datapoints, the volume of differentiable local minima is exponentially vanishing in comparison with the same volume of global minima. 
We propose an attention-invariant attack method to generate more transferable adversarial examples. 
We present Merged-Averaged Classifiers via Hashing (MACH) for $K$-classification with large $K$. 
A general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. 
We propose a novel test for estimating support size using the birthday paradox of discrete probability and show that well-known GANs approaches do learn distributions of fairly low support. 
We propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. 
Kronecker-factor Approximate Curvature is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks. 
In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained neural networks e.g., spatial correlations of weights. 
The high dimensionality of hyperspectral imaging forces unique challenges in scope, size and processing requirements. 
A new dataset containing $9.7$ million question-answer pairs grounded over $270,000$ plots with three main differentiators. 
In this paper, we extend the theory of Random Features to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster to train. 
We propose â€œfidelity-weighted learningâ€ (FWL), a semi-supervised student- teacher approach for training deep neural networks using weakly-labeled data. 
We propose new online learning approaches for supervised dimension reduction. 
This paper presents a storage-efficient learning model titled Recursive Binary Neural Networks for embedded and mobile devices having a limited amount of on-chip data storage. 
We propose two transfer learning methods that use this generative manifold representation to learn natural transformations and incorporate them into new data. 
This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. 
We propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. 
We extend the graph clustering problem to the graph down-sampling problem and we improve the state-of-the-art result for several data-sets. 
Adversarial examples generated by generative adversarial networks (GANs). 
This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. 
We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. 
We present a framework for interpretable continual learning (ICL). We show that explanations of previously performed tasks can be used to improve performance on future tasks. 
In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware. 
In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. 
We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces an inverse mapping from data to the latent space. 
We propose a method for learning the dependency structure between latent variables in deep latent variable models. 
We show how changepoint detection can be treated as a supervised learning problem, and propose a deep neural network architecture that can efficiently identify both abrupt and gradual changes at multiple scales. 
We learn efficient heuristics for automated reasoning algorithms through deep reinforcement learning. 
We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data. 
We propose to leverage automatic search techniques to discover new activation functions. 
We introduce a novel bottom-up approach to expand representations in fixed-depth architectures. 
Initialized Equilibrium Propagation, a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. 
We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. 
We introduce a latent representation from an arbitrary set of frames that can be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points. 
We show that the ADAM optimizer is a combination of two aspects: for each weight, the update direction is determined by the sign of the stochastic gradient, whereas the update magnitude is solely determined by an estimate of its relative variance. 
We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. 
In this paper, we address several limitations of the baseline negated architecture by proposing two further optimized architectures: a coarser-grained gated architecture employing (feature) group-level fusion weights and a two-stage gated architectures leveraging both the group- level and feature- level fusion weights. 
We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. 
NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. 
We introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. 
We propose noise contrastive priors to obtain reliable uncertainty estimates of neural network predictions. 
While object recognition in humans relies on analysing shape, CNNs do not have such a shape-bias. 
We propose a generative model and a conditional variant built on such a disentangled latent space. 
We extend a loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters. 
We develop a novel policy gradient method for the automatic learning of policies with options. 
The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. 
In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. 
This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. 
We explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. 
An optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on. 
We introduce geomstats, a Python package for Riemannian modelization and optimization over manifolds such as hyperspheres, hyperbolic spaces, SPD matrices or Lie groups of transformations. 
We propose to execute deep neural networks with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. 
We propose a novel, tractable approximation of Information-Directed Sampling for exploration in reinforcement learning. 
We address the problem of learning structured policies for continuous control. 
We propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. 
We use hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). 
We propose two compression algorithms: the first allows a user to specify the proportion of spectral energy that should be preserved in each layer after compression, while the second is a heuristic that automatically selects the compression used at each layer. 
We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. 
Conditional Network Embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc). 
This paper studies a class of adaptive gradient based momentum algorithms that update the  search directions and learning rates simultaneously using past gradients. 
This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. 
A Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer architecture to effectively model long-range dependencies between entity mentions. 
A new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. 
We propose GraphGAN - the first implicit generative model for graphs that enables to mimic real-world networks. 
We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. 
We present a flexible and interpretable framework for learning domain invariant speaker embeddings using Generative Adversarial Networks. 
Learning disentangling representations of the independent factors of variations that explain the data in an unsupervised setting. 
In this work, we propose the use of structured arrangements through randomized microbatches of examples that are more likely to include similar ones. 
We proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address the large number of out-of-vocabulary words. 
We propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. 
In this paper, we propose a novel asynchronous distributed algorithm that tackles this limitation by well-thought-out averaging of model updates, computed by workers. 
This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. 
We extend existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. 
We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. 
We propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE. 
We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters. 
We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. 
We investigate the role of residual and highway connections in deep neural networks for speech enhancement, and verify whether  or not they operate similarly to their traditional, digital signal processing  counterparts. 
We introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. 
We provide theoretical results for our composition technique and evaluate on a simple grid world simulation as well as a robotic manipulation task. 
The application of multi-modal generative models by means of a Variational Auto Encoder (VAE). 
A method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. 
In this work, we provide a two-timescale network (TTN) architecture that enables linear methods to be used to learn values, with a nonlinear representation learned at a slower timescale. 
In this paper, we propose simple, but effective, low-rank matrix factorization (MF) algorithms to compress network parameters and significantly speed up LSTMs with almost no loss of performance. 
This paper addresses the problem of determining if, given two images, one is a manipulated version of the other by means of certain geometric and statistical manipulations, e.g. copy, rotation, translation, perspective transform, histogram adjustment, partial erasing, and compression artifacts. 
We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. 
We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces. 
We propose to add conditional independence to the framework of fully-connected CRFs, which allows us to reformulate the inference in terms of convolutions, which can be implemented highly efficiently on GPUs. 
In this paper we propose a framework which validates robustness of any Question Answering model through model explainers. 
In this paper, we propose a mix-generator generative adversarial networks (PGAN) model that works in parallel by mixing multiple disjoint generators to approximate a complex real distribution. 
We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. 
We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. 
We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. 
We show that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. 
The robust and efficient recognition of visual relations in images is a hallmark of biological vision, but modern machine vision algorithms are severely limited in their ability to learn visual relations. 
In this paper, we propose a novel adversarial RL method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. 
We propose a method to learn a hierarchical word embedding in a speciï¬c order to capture the hypernymy. 
We show how gradient learning results in a form of a self-organizing learning rule. 
We propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low precision computation. 
We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data. 
Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y. 
We prove, under two sufficient conditions, that idealised models can have no adversarial examples. 
We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification of MNIST and CIFAR-10 examples. 
In this paper, we propose a measure of generalization gap, based on the concept of margin distribution, which are the distances of training points to the decision boundary. 
We propose a new algorithm to learn a one-hidden-layer neural network where both the convolutional weights and the outputs weights are parameters to be learned. 
We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. 
We introduce a new method for training GANs by applying the Wasserstein-2 metric proximal on the generators. 
In this paper, we develop a weighted mini-bucket approach for bounding the MEU. 
We revisit a feed-forward propagation approach that allows one to estimate for each neuron its mean and variance w.r.t. all mentioned sources of stochasticity. 
Any GAN objective, including Wasserstein GANs, benefit from adversarial robustness both quantitatively and qualitatively. 
We propose a method to learn stochastic activation functions for use in probabilistic neural networks. 
Deep diagonal-circulant ReLU networks of bounded width and small depth can approximate a deep ReLU network in which the dense matrices are of low rank. 
We designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest. 
In this paper, we propose a model, called "bi-directional block self-attention network (Bi-BloSAN)", for RNN/CNN-free sequence encoding, which requires as little memory as RNN but with all the merits of SAN. 
We propose the Coarse-grain Fine-grain Coattention Network (CFC), a question answering model that combines information from evidence across multiple documents. 
We present a scalable method for unbalanced optimal transport based on the generative-adversarial framework. 
We propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance. 
We propose a novel method, coined instance-aware GAN (InstaGAN), that incorporates the instance information (e.g., object segmentation masks) and improves multi-instance transfiguration. 
We show that the parameter-function map of many DNNs should be exponentially biased towards simple functions. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. 
A theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables. 
In this paper, we propose our Recurrent Distribution Regression Network (RDRN) which adopts a recurrent architecture for DRN. 
We present graph attention networks, novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. 
We present a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. 
We propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. 
We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. 
This paper outlines a new approach for learning underlying game state embeddings irrespective of the visual rendering of the game state. 
We study discrete time dynamical systems governed by the state equation $h_{t+1}=Ï•(Ah_t+Bu_t)$. 
We show that, by properly defining the neuron manifold of deep neuron network (DNN), we can significantly improve the performance of student DNN networks. 
We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. 
Non-metric distances are able to generate a more complex and accurate similarity model than metric distances, provided that the non-linear data distribution is precisely captured. 
An augmented cyclic adversarial learning model that enforces the cycle-consistency constraint via an external task specific model, which encourages the preservation of task-relevant content as opposed to exact reconstruction. 
We prove that nodes with similar local network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network. 
UniNet is a driving simulator that allows users to interact with and visualize simulated traffic in mixed reality. 
We propose to use more efficient numerical integration technique to obtain better estimates of the integrals compared to the state-of-the-art methods. 
We learn fine-grained correspondences between language and contextually relevant geometric properties of 3D objects. 
We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. 
We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. 
Recurrent auto-encoder model can summarise sequential data into a fixed-length vector and then reconstruct into its original sequential form through the decoder structure. 
We view molecule optimization as a graph-to-graph translation problem and propose a novel adversarial training method for aligning distributions of molecules. 
We propose an approach to learn a fast iterative solver tailored to a specific domain. 
We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes. 
In this paper, justified by the notion of delta-hyperbolicity, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. 
We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings. 
We compute deep convolutional network generators by inverting a fixed embedding operator. 
We present the first neural speed reading model to both skip and jump text during inference. 
In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). 
Stochastic learning algorithms can generalize well as long as their sensitiveness to adversarial perturbations is bounded in average over training examples. 
We show how neural attention and meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation on the Omniglot dataset. 
We prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. 
We propose to learn about decision states from prior experience by examining where the model accesses the goal state through the bottleneck. 
We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. 
This paper studies the unsupervised problem of a generative model exploiting graph convolution. 
We use the implicit regularization effect of stochastic gradient descent with large learning rates to identify and on-the-fly discard mislabeled examples. 
In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. 
We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. 
We introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically. 
We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. 
We show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. 
In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. 
We show improvements over SeqGAN and the model trained with MLE on synthetic data and real text data. 
We explore a novel strategy for decomposing generation for complicated objects in which we first generate latent variables which describe a subset of the observed variables. 
In this paper, we propose a new way to leverage visual knowledge for sentence representations. 
